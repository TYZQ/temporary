{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2-v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TYZQ/temporary/blob/master/lab2_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9QcGnGPdX2C"
      },
      "source": [
        "\n",
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9O3aM3Tb28q"
      },
      "source": [
        "!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo942LMOdlh4"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DokFOdD1dJEl"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import copy\n",
        "import pandas as pd\n",
        "\n",
        "# Visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDLJuIXK_vh"
      },
      "source": [
        "**Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5PkYfqfK_SA"
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 101    # 101 + 1: There is am extra Background class that should be removed \n",
        "\n",
        "BATCH_SIZE = 128\n",
        "#BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 1e-2            # The initial Learning Rate (di quanto mi sposto nella curva. LR piccolo->sposto molto)\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default \n",
        "\n",
        "NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwii0TBHvzh"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUDdw4j2H0Mc"
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Normalizes tensor with mean and standard deviation of ImageNet\n",
        "                                      #transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      #transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                                                                         \n",
        "])\n",
        "\n",
        "# Define transforms for training set ONLY.\n",
        "augmentation_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                              transforms.CenterCrop(224),\n",
        "                                              #transforms.RandomRotation(degrees=15),\n",
        "                                              #transforms.ColorJitter(),\n",
        "                                              transforms.Grayscale(num_output_channels=3),\n",
        "                                              transforms.RandomHorizontalFlip(),\n",
        "                                              transforms.ToTensor(),\n",
        "                                              transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                 \n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qYIHPzYLY7i"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfVq_uDHLbsR"
      },
      "source": [
        "#import shutil\n",
        "#shutil.rmtree('./Caltech101', ignore_errors=True)\n",
        "\n",
        "# Clone github repository with data\n",
        "if not os.path.isdir('./Caltech101'):\n",
        "  !git clone https://github.com/lindaludovisi/ellelle.git\n",
        "  !mv 'ellelle' 'Caltech101'\n",
        "\n",
        "\n",
        "DATA_DIR = 'Caltech101/101_ObjectCategories' #ObjectCategories is the directory containing all images\n",
        "from Caltech101.caltech_dataset import Caltech #Caltech is a class in caltech_dataset.py\n",
        "\n",
        "\n",
        "# Prepare Pytorch train/test Datasets\n",
        "train_dataset = Caltech(DATA_DIR, split='train',  transform=train_transform)\n",
        "test_dataset = Caltech(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "\n",
        "#Create a list of all the indexes of the original train dataset\n",
        "original_indexes=list(range(len(train_dataset)))\n",
        "\n",
        "train_indexes = []\n",
        "val_indexes = []\n",
        "#Divide train dataset into train/val\n",
        "#The idea here is to assign indexes divisible by 2 to val_dataset and the others to train_dataset\n",
        "#The proportion between categories is maintained because the dataset is sorted by alphabetical order\n",
        "for index in original_indexes:\n",
        "  if ( (index %2) == 0) :  \n",
        "    val_indexes.append(index)\n",
        "  else:                    \n",
        "    train_indexes.append(index)\n",
        "\n",
        "val_dataset = Subset(train_dataset, val_indexes)\n",
        "train_dataset = Subset(train_dataset, train_indexes)\n",
        "\n",
        "#Perform data augmentation for train_dataset\n",
        "#train_dataset.dataset.transform = augmentation_transform\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "print('Valid Dataset: {}'.format(len(val_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYEDQ7Z21ldN"
      },
      "source": [
        "**Prepare Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VriRw8SI1nle"
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbZ1t5Qs2z4j"
      },
      "source": [
        "\n",
        "**Prepare Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exHUjtXa22DN"
      },
      "source": [
        "#A useful function that sets the parameters of the model to be updated in training phase.\n",
        "#When we are finetuning we can leave all of the .requires_gradâ€™s set to the default of True.\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "  \n",
        "  if feature_extracting == 'conv':  #Freeze conv layers\n",
        "    for name, param in model.named_parameters(): \n",
        "      if 'features' in name :\n",
        "        param.requires_grad = False\n",
        "  \n",
        "  elif feature_extracting == 'fc':  #Freeze fc layers\n",
        "    for name, param in model.named_parameters(): \n",
        "      if 'classifier' in name :\n",
        "        param.requires_grad = False\n",
        "        print(name)\n",
        "      \n",
        "  else:\n",
        "    return True\n",
        "\n",
        "\n",
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "  # Initialize these variables which will be set in this if statement. Each of these\n",
        "  # variables is model specific.\n",
        "  model_ft = None\n",
        "  input_size = 0\n",
        "\n",
        "  if model_name == \"vgg\":\n",
        "    \"\"\" VGG11_bn\n",
        "    \"\"\"\n",
        "    model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "    set_parameter_requires_grad(model_ft, feature_extract)\n",
        "    num_ftrs = model_ft.classifier[6].in_features  \n",
        "    model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
        "    input_size = 224\n",
        "\n",
        "  elif model_name == \"alexnet\":\n",
        "    \"\"\" Alexnet\n",
        "    \"\"\"\n",
        "    model_ft = models.alexnet(pretrained=use_pretrained)  \n",
        "    num_ftrs = model_ft.classifier[6].in_features\n",
        "    model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)  # nn.Linear in pytorch is a fully connected layer\n",
        "                                                              # The convolutional layer is nn.Conv2d\n",
        "    set_parameter_requires_grad(model_ft, feature_extract) \n",
        "    input_size = 224 \n",
        "\n",
        "  return model_ft, input_size\n",
        "\n",
        "\n",
        "# Loading AlexNet model (not pre-trained)\n",
        "#net, input_size = initialize_model(\"alexnet\", NUM_CLASSES, False, use_pretrained=False)\n",
        "\n",
        "# Loading AlexNet pre-trained model\n",
        "#net, input_size = initialize_model(\"alexnet\", NUM_CLASSES, feature_extract=False, use_pretrained=True)\n",
        "\n",
        "# Loading AlexNet pre-trained model, freezing conv layers\n",
        "#net, input_size = initialize_model(\"alexnet\", NUM_CLASSES, feature_extract='conv', use_pretrained=True)\n",
        "\n",
        "# Loading AlexNet pre-trained model, freezing fc layers\n",
        "#net, input_size = initialize_model(\"alexnet\", NUM_CLASSES, feature_extract='fc', use_pretrained=True)\n",
        "\n",
        "# Loading VGG pre-trained model, freezing conv layers\n",
        "net, input_size = initialize_model(\"vgg\", NUM_CLASSES, feature_extract='conv', use_pretrained=True)\n",
        "\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(net)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEyL3H_R4qCf"
      },
      "source": [
        "**Prepare Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sjq00G94tSc"
      },
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "\n",
        "\n",
        "# Choose parameters to optimize\n",
        " \n",
        "#parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet\n",
        "\n",
        "parameters_to_optimize = []\n",
        "print(\"Params to learn:\")\n",
        "for name, param in net.named_parameters():\n",
        "  if param.requires_grad == True:\n",
        "    parameters_to_optimize.append(param)\n",
        "    print(\"\\t\",name)\n",
        "\n",
        "\n",
        "# Define optimizer\n",
        "# An optimizer updates the weights based on loss\n",
        "\n",
        "# We use SGD with momentum\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "#We also try to use Adam optimizer\n",
        "#optimizer = optim.Adam(parameters_to_optimize, lr=LR, weight_decay=WEIGHT_DECAY )\n",
        "\n",
        "\n",
        "# Define scheduler\n",
        "# A scheduler dynamically changes learning rate\n",
        "# The most common schedule is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxYUli9d9uYQ"
      },
      "source": [
        "**Train and Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcoQ5fD49yT_"
      },
      "source": [
        "# By default, everything is loaded to cpu\n",
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "history = []\n",
        "val_acc_history = []\n",
        "best_acc = 0\n",
        "current_step = 0\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "  # keep track of training and validation loss each epoch\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "\n",
        "  #\n",
        "  #   TRAINING\n",
        "  #\n",
        "\n",
        "  # Iterate over the dataset\n",
        "  for images, labels in train_dataloader:\n",
        "    # Bring data over the device of choice\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    net.train() # Sets module in training mode\n",
        "\n",
        "    # Clear the gradients\n",
        "    optimizer.zero_grad() \n",
        "\n",
        "    # Forward pass to the network\n",
        "    outputs = net(images)\n",
        "\n",
        "    # Compute loss based on output and ground truth\n",
        "    loss = criterion(outputs, labels)\n",
        "    # Backpropagation of gradients\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update the weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Track train loss by multiplying average loss by number of examples in batch\n",
        "    train_loss += loss.item() * images.size(0)   \n",
        "\n",
        "    # Log loss\n",
        "    if current_step % LOG_FREQUENCY == 0:\n",
        "      print('Step {}, Train Loss {}'.format(current_step, loss.item()))    \n",
        "\n",
        "    current_step += 1\n",
        "\n",
        "  # Step the scheduler\n",
        "  scheduler.step() \n",
        "\n",
        "  \n",
        "  #\n",
        "  #   VALIDATION\n",
        "  #\n",
        "  \n",
        "  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "  net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "  running_corrects = 0\n",
        "  for images, labels in tqdm(val_dataloader):\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    # Forward Pass \n",
        "    outputs = net(images)\n",
        "\n",
        "    # Validation loss\n",
        "    loss = criterion(outputs, labels)\n",
        "    # Multiply average loss times the number of examples in batch\n",
        "    valid_loss += loss.item() * images.size(0)\n",
        "\n",
        "    # Get predictions from the maximum value\n",
        "    _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "    # Update Corrects\n",
        "    running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "  # Calculate Accuracy\n",
        "  accuracy = running_corrects / float(len(val_dataset))\n",
        "\n",
        "  print('Validation Accuracy: {}'.format(accuracy))\n",
        "\n",
        "  # Append the value of the accuracy to the list\n",
        "  val_acc_history.append(accuracy)\n",
        "\n",
        "  #Save the model with the best accuracy\n",
        "  if (accuracy > best_acc):\n",
        "    best_acc = accuracy\n",
        "    best_net = copy.deepcopy(net.state_dict())\n",
        "\n",
        "  # Calculate average losses\n",
        "  train_loss = train_loss / len(train_dataloader.dataset)\n",
        "  valid_loss = valid_loss / len(val_dataloader.dataset)\n",
        "  \n",
        "  #Save train and validation loss\n",
        "  history.append([train_loss, valid_loss])\n",
        "\n",
        "#at the end, load best model weights\n",
        "net.load_state_dict(best_net)\n",
        "\n",
        "print(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsHFI-GAJd69"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO3HV5pqJg1o"
      },
      "source": [
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(val_dataloader):\n",
        "#for images, labels in val_dataloader:\n",
        "  images = images.to(DEVICE)\n",
        "  labels = labels.to(DEVICE)\n",
        "\n",
        "  # Forward Pass in order to get logits/output\n",
        "  outputs = net(images)\n",
        "\n",
        "  # Get predictions from the maximum value\n",
        "  _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "  # Update Corrects\n",
        "  running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = running_corrects / float(len(val_dataset))\n",
        "\n",
        "print('Validation Accuracy performed on the best model: {}'.format(accuracy))\n",
        "\n",
        "\n",
        "# History is a Dataframe\n",
        "history = pd.DataFrame( history, columns=['train_loss', 'valid_loss'])\n",
        "\n",
        "# Plot train_loss vs valid_loss\n",
        "plt.figure(figsize=(8, 6))\n",
        "for c in ['train_loss', 'valid_loss']:\n",
        "  plt.plot(history[c], label=c)\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Loss')\n",
        "plt.title(f'Training and Validation Losses using SGD optimizer, LR={LR}, epochs={NUM_EPOCHS}')\n",
        "\n",
        "from torchsummary import summary\n",
        "summary(\n",
        "        net, input_size=(3, 224, 224), batch_size=BATCH_SIZE, device='cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxekmR745ySe"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSHcUqLB5yWO"
      },
      "source": [
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(test_dataloader):\n",
        "  images = images.to(DEVICE)\n",
        "  labels = labels.to(DEVICE)\n",
        "\n",
        "  # Forward Pass\n",
        "  outputs = net(images)\n",
        "\n",
        "  # Get predictions\n",
        "  _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "  # Update Corrects\n",
        "  running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "print('Test Accuracy: {}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}